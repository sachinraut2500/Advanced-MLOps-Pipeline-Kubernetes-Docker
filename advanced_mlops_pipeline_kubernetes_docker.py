# -*- coding: utf-8 -*-
"""Advanced-MLOps-Pipeline-Kubernetes-Docker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pKYgrwW1BbCxeOcU-7lO2w31ebT7-_yO
"""

### mlops_pipeline.ipynb
```python
# Advanced MLOps Pipeline Implementation
import os
import json
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import joblib
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_classification
import warnings
warnings.filterwarnings('ignore')

print("Advanced MLOps Pipeline Implementation")
print("=" * 50)

# 1. Model Development and Training Pipeline
class MLModelPipeline:
    def __init__(self, model_name="ml_classifier"):
        self.model_name = model_name
        self.model = None
        self.scaler = StandardScaler()
        self.metrics = {}
        self.version = "1.0.0"
        self.metadata = {}

    def prepare_data(self, X, y):
        """Data preparation and preprocessing"""
        print("Preparing data...")
        X_scaled = self.scaler.fit_transform(X)
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )
        return X_train, X_test, y_train, y_test

    def train_model(self, X_train, y_train, model_type="random_forest"):
        """Train the machine learning model"""
        print(f"Training {model_type} model...")

        if model_type == "random_forest":
            self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        elif model_type == "gradient_boosting":
            self.model = GradientBoostingClassifier(n_estimators=100, random_state=42)

        # Cross-validation
        cv_scores = cross_val_score(self.model, X_train, y_train, cv=5)
        self.model.fit(X_train, y_train)

        self.metadata.update({
            'training_time': datetime.now().isoformat(),
            'model_type': model_type,
            'cv_mean_score': cv_scores.mean(),
            'cv_std_score': cv_scores.std(),
            'training_samples': len(X_train)
        })

        print(f"Model trained. CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

    def evaluate_model(self, X_test, y_test):
        """Evaluate model performance"""
        print("Evaluating model...")
        y_pred = self.model.predict(X_test)

        self.metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, average='weighted'),
            'recall': recall_score(y_test, y_pred, average='weighted'),
            'f1_score': f1_score(y_test, y_pred, average='weighted')
        }

        print("Model Performance:")
        for metric, value in self.metrics.items():
            print(f"{metric.capitalize()}: {value:.4f}")

        return self.metrics

    def save_model(self, model_dir="models"):
        """Save model artifacts"""
        os.makedirs(model_dir, exist_ok=True)

        model_path = os.path.join(model_dir, f"{self.model_name}_v{self.version}.pkl")
        scaler_path = os.path.join(model_dir, f"{self.model_name}_scaler_v{self.version}.pkl")
        metadata_path = os.path.join(model_dir, f"{self.model_name}_metadata_v{self.version}.json")

        # Save model and scaler
        joblib.dump(self.model, model_path)
        joblib.dump(self.scaler, scaler_path)

        # Save metadata
        complete_metadata = {
            'model_name': self.model_name,
            'version': self.version,
            'metrics': self.metrics,
            'metadata': self.metadata,
            'model_path': model_path,
            'scaler_path': scaler_path
        }

        with open(metadata_path, 'w') as f:
            json.dump(complete_metadata, f, indent=2)

        print(f"Model saved: {model_path}")
        return model_path, scaler_path, metadata_path

# 2. Model Registry and Versioning
class ModelRegistry:
    def __init__(self, registry_path="model_registry"):
        self.registry_path = registry_path
        os.makedirs(registry_path, exist_ok=True)
        self.registry_file = os.path.join(registry_path, "registry.json")
        self.load_registry()

    def load_registry(self):
        """Load model registry"""
        if os.path.exists(self.registry_file):
            with open(self.registry_file, 'r') as f:
                self.registry = json.load(f)
        else:
            self.registry = {"models": {}}

    def register_model(self, model_name, version, metadata):
        """Register a new model version"""
        if model_name not in self.registry["models"]:
            self.registry["models"][model_name] = {"versions": {}}

        self.registry["models"][model_name]["versions"][version] = {
            "registered_at": datetime.now().isoformat(),
            "metadata": metadata,
            "status": "registered"
        }

        self.save_registry()
        print(f"Model {model_name} v{version} registered successfully")

    def promote_model(self, model_name, version, stage="production"):
        """Promote model to different stage"""
        if model_name in self.registry["models"] and version in self.registry["models"][model_name]["versions"]:
            self.registry["models"][model_name]["versions"][version]["status"] = stage
            self.save_registry()
            print(f"Model {model_name} v{version} promoted to {stage}")
        else:
            print(f"Model {model_name} v{version} not found in registry")

    def save_registry(self):
        """Save registry to file"""
        with open(self.registry_file, 'w') as f:
            json.dump(self.registry, f, indent=2)

    def list_models(self):
        """List all registered models"""
        print("Registered Models:")
        for model_name, model_info in self.registry["models"].items():
            print(f"\n{model_name}:")
            for version, version_info in model_info["versions"].items():
                status = version_info["status"]
                accuracy = version_info["metadata"]["metrics"]["accuracy"]
                print(f"  v{version} - Status: {status} - Accuracy: {accuracy:.4f}")

# 3. Model Serving Infrastructure
class ModelServer:
    def __init__(self, model_path, scaler_path):
        self.model = joblib.load(model_path)
        self.scaler = joblib.load(scaler_path)
        self.prediction_count = 0
        self.prediction_history = []

    def predict(self, features):
        """Make prediction with monitoring"""
        features_scaled = self.scaler.transform([features])
        prediction = self.model.predict(features_scaled)[0]
        probability = self.model.predict_proba(features_scaled)[0].max()

        # Log prediction
        self.prediction_count += 1
        self.prediction_history.append({
            'timestamp': datetime.now().isoformat(),
            'prediction': int(prediction),
            'probability': float(probability),
            'features': features
        })

        return {
            'prediction': int(prediction),
            'probability': float(probability),
            'model_version': '1.0.0'
        }

    def get_health_metrics(self):
        """Get server health metrics"""
        return {
            'prediction_count': self.prediction_count,
            'uptime': datetime.now().isoformat(),
            'model_loaded': self.model is not None,
            'average_probability': np.mean([p['probability'] for p in self.prediction_history[-100:]]) if self.prediction_history else 0
        }

# 4. Monitoring and Alerting
class ModelMonitor:
    def __init__(self):
        self.performance_history = []
        self.drift_metrics = []
        self.alert_thresholds = {
            'accuracy_drop': 0.05,
            'prediction_latency': 1.0,
            'error_rate': 0.1
        }

    def log_performance(self, y_true, y_pred, timestamp=None):
        """Log model performance metrics"""
        if timestamp is None:
            timestamp = datetime.now()

        metrics = {
            'timestamp': timestamp.isoformat(),
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, average='weighted'),
            'recall': recall_score(y_true, y_pred, average='weighted'),
            'f1_score': f1_score(y_true, y_pred, average='weighted')
        }

        self.performance_history.append(metrics)
        return metrics

    def detect_drift(self, current_data, reference_data):
        """Simple data drift detection"""
        # Statistical tests for drift detection
        from scipy import stats

        drift_scores = []
        for i in range(current_data.shape[1]):
            # Kolmogorov-Smirnov test
            ks_stat, p_value = stats.ks_2samp(reference_data[:, i], current_data[:, i])
            drift_scores.append({
                'feature': i,
                'ks_statistic': ks_stat,
                'p_value': p_value,
                'drift_detected': p_value < 0.05
            })

        drift_summary = {
            'timestamp': datetime.now().isoformat(),
            'features_with_drift': sum(1 for score in drift_scores if score['drift_detected']),
            'total_features': len(drift_scores),
            'drift_details': drift_scores
        }

        self.drift_metrics.append(drift_summary)
        return drift_summary

    def generate_alerts(self):
        """Generate alerts based on thresholds"""
        alerts = []

        if len(self.performance_history) >= 2:
            current = self.performance_history[-1]
            previous = self.performance_history[-2]

            accuracy_drop = previous['accuracy'] - current['accuracy']
            if accuracy_drop > self.alert_thresholds['accuracy_drop']:
                alerts.append({
                    'type': 'PERFORMANCE_DEGRADATION',
                    'message': f"Accuracy dropped by {accuracy_drop:.4f}",
                    'severity': 'HIGH',
                    'timestamp': datetime.now().isoformat()
                })

        return alerts

# 5. A/B Testing Framework
class ABTestFramework:
    def __init__(self):
        self.experiments = {}
        self.results = {}

    def create_experiment(self, experiment_name, model_a_path, model_b_path, traffic_split=0.5):
        """Create A/B testing experiment"""
        self.experiments[experiment_name] = {
            'model_a': joblib.load(model_a_path),
            'model_b': joblib.load(model_b_path),
            'traffic_split': traffic_split,
            'predictions_a': [],
            'predictions_b': [],
            'created_at': datetime.now().isoformat()
        }
        print(f"A/B experiment '{experiment_name}' created")

    def route_prediction(self, experiment_name, features):
        """Route prediction to model A or B based on traffic split"""
        if experiment_name not in self.experiments:
            raise ValueError(f"Experiment {experiment_name} not found")

        experiment = self.experiments[experiment_name]

        # Random traffic routing
        if np.random.random() < experiment['traffic_split']:
            prediction = experiment['model_a'].predict([features])[0]
            experiment['predictions_a'].append({
                'features': features,
                'prediction': prediction,
                'timestamp': datetime.now().isoformat()
            })
            return prediction, 'model_a'
        else:
            prediction = experiment['model_b'].predict([features])[0]
            experiment['predictions_b'].append({
                'features': features,
                'prediction': prediction,
                'timestamp': datetime.now().isoformat()
            })
            return prediction, 'model_b'

    def analyze_experiment(self, experiment_name, y_true_a, y_true_b):
        """Analyze A/B test results"""
        if experiment_name not in self.experiments:
            return None

        experiment = self.experiments[experiment_name]

        # Get predictions
        y_pred_a = [p['prediction'] for p in experiment['predictions_a']]
        y_pred_b = [p['prediction'] for p in experiment['predictions_b']]

        if len(y_pred_a) == 0 or len(y_pred_b) == 0:
            print("Insufficient data for analysis")
            return None

        # Calculate metrics
        metrics_a = {
            'accuracy': accuracy_score(y_true_a[:len(y_pred_a)], y_pred_a),
            'sample_size': len(y_pred_a)
        }

        metrics_b = {
            'accuracy': accuracy_score(y_true_b[:len(y_pred_b)], y_pred_b),
            'sample_size': len(y_pred_b)
        }

        # Statistical significance test
        from scipy import stats
        z_score, p_value = stats.proportions_ztest(
            [metrics_a['accuracy'] * metrics_a['sample_size'],
             metrics_b['accuracy'] * metrics_b['sample_size']],
            [metrics_a['sample_size'], metrics_b['sample_size']]
        )

        results = {
            'experiment_name': experiment_name,
            'model_a_metrics': metrics_a,
            'model_b_metrics': metrics_b,
            'statistical_significance': {
                'z_score': z_score,
                'p_value': p_value,
                'significant': p_value < 0.05
            },
            'winner': 'model_a' if metrics_a['accuracy'] > metrics_b['accuracy'] else 'model_b'
        }

        self.results[experiment_name] = results
        return results

# Demo Implementation
print("Starting MLOps Pipeline Demo...")

# Generate sample data
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,
                          n_redundant=5, n_classes=2, random_state=42)

# 1. Train models
pipeline_a = MLModelPipeline("classifier_a")
X_train, X_test, y_train, y_test = pipeline_a.prepare_data(X, y)
pipeline_a.train_model(X_train, y_train, "random_forest")
metrics_a = pipeline_a.evaluate_model(X_test, y_test)
model_path_a, scaler_path_a, metadata_path_a = pipeline_a.save_model()

pipeline_b = MLModelPipeline("classifier_b")
pipeline_b.scaler = pipeline_a.scaler  # Use same scaler
pipeline_b.train_model(X_train, y_train, "gradient_boosting")
metrics_b = pipeline_b.evaluate_model(X_test, y_test)
model_path_b, scaler_path_b, metadata_path_b = pipeline_b.save_model()

# 2. Model Registry
registry = ModelRegistry()
registry.register_model("classifier_a", "1.0.0", {
    "metrics": metrics_a,
    "model_path": model_path_a
})
registry.register_model("classifier_b", "1.0.0", {
    "metrics": metrics_b,
    "model_path": model_path_b
})
registry.promote_model("classifier_a", "1.0.0", "production")
registry.list_models()

# 3. Model Serving
server = ModelServer(model_path_a, scaler_path_a)

# Make some predictions
sample_predictions = []
for i in range(10):
    features = X_test[i].tolist()
    result = server.predict(features)
    sample_predictions.append(result)
    print(f"Prediction {i+1}: {result}")

# 4. Monitoring
monitor = ModelMonitor()

# Simulate performance monitoring
y_pred_sample = [p['prediction'] for p in sample_predictions]
y_true_sample = y_test[:10]
performance = monitor.log_performance(y_true_sample, y_pred_sample)
print(f"\nCurrent Performance: {performance}")

# Drift detection
reference_data = X_train[:100]
current_data = X_test[:50]
drift_result = monitor.detect_drift(current_data, reference_data)
print(f"\nDrift Detection: {drift_result['features_with_drift']} features showing drift")

# 5. A/B Testing
ab_test = ABTestFramework()
ab_test.create_experiment("model_comparison", model_path_a, model_path_b)

# Simulate A/B test traffic
ab_predictions = []
for i in range(20):
    features = X_test[i]
    prediction, model_used = ab_test.route_prediction("model_comparison", features)
    ab_predictions.append((prediction, model_used))

print(f"\nA/B Test Routing: {len([p for p in ab_predictions if p[1] == 'model_a'])} to Model A, "
      f"{len([p for p in ab_predictions if p[1] == 'model_b'])} to Model B")

# Visualization
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# Model comparison
models = ['Random Forest', 'Gradient Boosting']
accuracies = [metrics_a['accuracy'], metrics_b['accuracy']]
axes[0, 0].bar(models, accuracies, color=['blue', 'orange'])
axes[0, 0].set_title('Model Performance Comparison')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].set_ylim(0, 1)

# Prediction confidence distribution
confidences = [p['probability'] for p in sample_predictions]
axes[0, 1].hist(confidences, bins=10, alpha=0.7, color='green')
axes[0, 1].set_title('Prediction Confidence Distribution')
axes[0, 1].set_xlabel('Confidence Score')
axes[0, 1].set_ylabel('Frequency')

# Server metrics over time
timestamps = list(range(len(sample_predictions)))
predictions = [p['prediction'] for p in sample_predictions]
axes[1, 0].plot(timestamps, predictions, 'o-', color='red')
axes[1, 0].set_title('Predictions Over Time')
axes[1, 0].set_xlabel('Request Number')
axes[1, 0].set_ylabel('Prediction')

# A/B test distribution
model_counts = {'Model A': len([p for p in ab_predictions if p[1] == 'model_a']),
                'Model B': len([p for p in ab_predictions if p[1] == 'model_b'])}
axes[1, 1].pie(model_counts.values(), labels=model_counts.keys(), autopct='%1.1f%%')
axes[1, 1].set_title('A/B Test Traffic Distribution')

plt.tight_layout()
plt.show()

print("\n=== MLOps Pipeline Summary ===")
print("Components Implemented:")
print("✓ Model training and evaluation pipeline")
print("✓ Model registry and versioning")
print("✓ Model serving infrastructure")
print("✓ Performance monitoring and drift detection")
print("✓ A/B testing framework")
print("✓ Automated alerting system")

print(f"\nHealth Metrics: {server.get_health_metrics()}")
print("MLOps Pipeline Implementation Complete!")

# Generate Docker and Kubernetes configuration files
docker_config = '''
FROM python:3.9-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "model_server:app", "--host", "0.0.0.0", "--port", "8000"]
'''

kubernetes_config = '''
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ml-model-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ml-model
  template:
    metadata:
      labels:
        app: ml-model
    spec:
      containers:
      - name: ml-model
        image: ml-model:latest
        ports:
        - containerPort: 8000
        env:
        - name: MODEL_VERSION
          value: "1.0.0"
        resources:
          limits:
            memory: "1Gi"
            cpu: "500m"
          requests:
            memory: "512Mi"
            cpu: "250m"
---
apiVersion: v1
kind: Service
metadata:
  name: ml-model-service
spec:
  selector:
    app: ml-model
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
'''

print(f"\nDocker Configuration:\n{docker_config}")
print(f"\nKubernetes Configuration:\n{kubernetes_config}")